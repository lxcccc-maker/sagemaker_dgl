{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f462cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dgl in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from dgl) (1.19.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from dgl) (2.26.0)\n",
      "Requirement already satisfied: networkx>=2.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from dgl) (2.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from dgl) (1.5.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from networkx>=2.1->dgl) (5.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl) (2.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9763cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=4, num_edges=4,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n",
      "tensor([0, 1, 2, 3])\n",
      "(tensor([0, 0, 0, 1]), tensor([1, 2, 3, 3]))\n",
      "(tensor([0, 0, 0, 1]), tensor([1, 2, 3, 3]), tensor([0, 1, 2, 3]))\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "import torch as th\n",
    "\n",
    "# edges 0->1, 0->2, 0->3, 1->3\n",
    "\n",
    "u, v = th.tensor([0, 0, 0, 1]), th.tensor([1, 2, 3, 3])\n",
    "\n",
    "g = dgl.graph((u, v))\n",
    "\n",
    "print(g) # number of nodes are inferred from the max node IDs in the given edges\n",
    "\n",
    "# Node IDs\n",
    "\n",
    "print(g.nodes())\n",
    "\n",
    "# Edge end nodes\n",
    "\n",
    "print(g.edges())\n",
    "\n",
    "# Edge end nodes and edge IDs\n",
    "\n",
    "print(g.edges(form='all'))\n",
    "\n",
    "# If the node with the largest ID is isolated (meaning no edges),\n",
    "\n",
    "# then one needs to explicitly set the number of nodes\n",
    "\n",
    "g = dgl.graph((u, v), num_nodes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f9bf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = th.tensor([2, 5, 3]), th.tensor([3, 5, 0])  # edges 2->3, 5->5, 3->0\n",
    "g64 = dgl.graph(edges)  # DGL uses int64 by default\n",
    "print(g64.idtype)\n",
    "g32 = dgl.graph(edges, idtype=th.int32)  # create a int32 graph\n",
    "g32.idtype\n",
    "g64_2 = g32.long()  # convert to int64\n",
    "g64_2.idtype\n",
    "g32_2 = g64.int()  # convert to int32\n",
    "g32_2.idtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "097da004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=6, num_edges=3,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    }
   ],
   "source": [
    "g = dgl.heterograph({\n",
    "   ('drug', 'interacts', 'drug'): (th.tensor([0, 1]), th.tensor([1, 2])),\n",
    "   ('drug', 'treats', 'disease'): (th.tensor([1]), th.tensor([2]))})\n",
    "g.nodes['drug'].data['hv'] = th.ones(3, 4)\n",
    "g.nodes['disease'].data['hv'] = th.ones(3, 4)\n",
    "g.edges['interacts'].data['he'] = th.zeros(2, 1)\n",
    "g.edges['treats'].data['he'] = th.zeros(1, 2)\n",
    "# By default, it does not merge any features\n",
    "hg = dgl.to_homogeneous(g)\n",
    "'hv' in hg.ndata\n",
    "# Copy edge features\n",
    "# For feature copy, it expects features to have\n",
    "# the same size and dtype across node/edge types\n",
    "# hg = dgl.to_homogeneous(g, edata=['he'])\n",
    "# Copy node features\n",
    "hg = dgl.to_homogeneous(g) # , ndata=['hv']\n",
    "print(hg)\n",
    "g_feat = g.ndata['hv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bec3d601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "u, v = th.tensor([0, 1, 2]), th.tensor([2, 3, 4])\n",
    "g = dgl.graph((u, v))\n",
    "g.ndata['x'] = th.randn(5, 3)  # original feature is on CPU\n",
    "g.device\n",
    "# cuda_g = g.to('cuda:0')  # accepts any device objects from backend framework\n",
    "# cuda_g.device\n",
    "# cuda_g.ndata['x'].device       # feature data is copied to GPU too\n",
    "# # A graph constructed from GPU tensors is also on GPU\n",
    "# u, v = u.to('cuda:0'), v.to('cuda:0')\n",
    "# g = dgl.graph((u, v))\n",
    "# g.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b56b9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from dgl.utils import expand_as_pair\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "from dgl.utils import check_eq_shape\n",
    "\n",
    "class SAGEConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 aggregator_type,\n",
    "                 bias=True,\n",
    "                 norm=None,\n",
    "                 activation=None):\n",
    "        super(SAGEConv, self).__init__()\n",
    "\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._aggre_type = aggregator_type\n",
    "        self.norm = norm\n",
    "        self.activation = activation\n",
    "        # aggregator type: mean, pool, lstm, gcn\n",
    "        if aggregator_type not in ['mean', 'pool', 'lstm', 'gcn']:\n",
    "            raise KeyError('Aggregator type {} not supported.'.format(aggregator_type))\n",
    "        if aggregator_type == 'pool':\n",
    "            self.fc_pool = nn.Linear(self._in_src_feats, self._in_src_feats)\n",
    "        if aggregator_type == 'lstm':\n",
    "            self.lstm = nn.LSTM(self._in_src_feats, self._in_src_feats, batch_first=True)\n",
    "        if aggregator_type in ['mean', 'pool', 'lstm']:\n",
    "            self.fc_self = nn.Linear(self._in_dst_feats, out_feats, bias=bias)\n",
    "        self.fc_neigh = nn.Linear(self._in_src_feats, out_feats, bias=bias)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "   \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        if self._aggre_type == 'pool':\n",
    "            nn.init.xavier_uniform_(self.fc_pool.weight, gain=gain)\n",
    "        if self._aggre_type == 'lstm':\n",
    "            self.lstm.reset_parameters()\n",
    "        if self._aggre_type != 'gcn':\n",
    "            nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)\n",
    "    \n",
    "    def forward(self, graph, feat):\n",
    "        with graph.local_scope():\n",
    "            # Specify graph type then expand input feature according to graph type\n",
    "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
    "            if self._aggre_type == 'mean':\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                graph.dstdata['h'] = feat_dst\n",
    "                graph.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'neigh'))\n",
    "                h_neigh = graph.dstdata['neigh']\n",
    "            elif self._aggre_type == 'gcn':\n",
    "                check_eq_shape(feat)\n",
    "                graph.srcdata['h'] = feat_src\n",
    "                graph.dstdata['h'] = feat_dst\n",
    "                graph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'neigh'))\n",
    "                # divide in_degrees\n",
    "                degs = graph.in_degrees().to(feat_dst)\n",
    "                h_neigh = (graph.dstdata['neigh'] + graph.dstdata['h']) / (degs.unsqueeze(-1) + 1)\n",
    "            elif self._aggre_type == 'pool':\n",
    "                graph.srcdata['h'] = F.relu(self.fc_pool(feat_src))\n",
    "                graph.update_all(fn.copy_u('h', 'm'), fn.max('m', 'neigh'))\n",
    "                h_neigh = graph.dstdata['neigh']\n",
    "            else:\n",
    "                raise KeyError('Aggregator type {} not recognized.'.format(self._aggre_type))\n",
    "\n",
    "            # GraphSAGE GCN does not require fc_self.\n",
    "            if self._aggre_type == 'gcn':\n",
    "                rst = self.fc_neigh(h_neigh)\n",
    "            else:\n",
    "                h_self = graph.dstdata['h']\n",
    "                rst = self.fc_self(h_self) + self.fc_neigh(h_neigh)\n",
    "        return rst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6741704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphSage = SAGEConv(4, 2, 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f40698ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=6, num_edges=3,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    }
   ],
   "source": [
    "print(hg)\n",
    "feat = th.ones(6,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc6706a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2324, -0.7498],\n",
      "        [ 2.2324, -0.7498],\n",
      "        [ 3.9787,  1.6786],\n",
      "        [ 2.2324, -0.7498],\n",
      "        [ 3.9787,  1.6786],\n",
      "        [ 3.9787,  1.6786]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dst_feat = graphSage(hg, feat.float())\n",
    "print(dst_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b8176e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_aggregate_fn(aggregator_type, in_feats, out_feats):\n",
    "    if aggregator_type not in ['mean', 'pool', 'lstm', 'gcn','sum']:\n",
    "        raise KeyError('Aggregator type {} not supported.'.format(aggregator_type))\n",
    "    if aggregator_type == 'pool':\n",
    "        return nn.Linear(in_feats, in_feats)\n",
    "    if aggregator_type == 'lstm':\n",
    "        return nn.LSTM(in_feats, in_feats, batch_first=True)\n",
    "    if aggregator_type in ['mean', 'pool', 'lstm']:\n",
    "        return nn.Linear(in_feats, out_feats, bias=True)\n",
    "    \n",
    "class HeteroGraphConv(nn.Module):\n",
    "    def __init__(self, mods,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 aggregate='sum'):\n",
    "        \n",
    "        super(HeteroGraphConv, self).__init__()\n",
    "        self.mods = nn.ModuleDict(mods)\n",
    "        if isinstance(aggregate, str):\n",
    "            # An internal function to get common aggregation functions\n",
    "            self.agg_fn = get_aggregate_fn(aggregate, in_feats, out_feats)\n",
    "        else:\n",
    "            self.agg_fn = aggregate\n",
    "            \n",
    "    def forward(self, g, inputs, mod_args=None, mod_kwargs=None):\n",
    "        if mod_args is None:\n",
    "            mod_args = {}\n",
    "        if mod_kwargs is None:\n",
    "            mod_kwargs = {}\n",
    "        outputs = {nty : [] for nty in g.dsttypes}\n",
    "        \n",
    "        if g.is_block:\n",
    "            src_inputs = inputs\n",
    "            dst_inputs = {k: v[:g.number_of_dst_nodes(k)] for k, v in inputs.items()}\n",
    "        else:\n",
    "            src_inputs = dst_inputs = inputs\n",
    "\n",
    "        for stype, etype, dtype in g.canonical_etypes:\n",
    "            rel_graph = g[stype, etype, dtype]\n",
    "            if rel_graph.num_edges() == 0:\n",
    "                continue\n",
    "            if stype not in src_inputs or dtype not in dst_inputs:\n",
    "                continue\n",
    "            dstdata = self.mods[etype](\n",
    "                rel_graph,\n",
    "                (src_inputs[stype], dst_inputs[dtype]),\n",
    "                *mod_args.get(etype, ()),\n",
    "                **mod_kwargs.get(etype, {}))\n",
    "            outputs[dtype].append(dstdata)\n",
    "        return outputs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2690a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_feats = 4\n",
    "out_feats = 2\n",
    "mods = {\n",
    "    'interacts' : SAGEConv(4, 2, 'mean'),\n",
    "    'treats' : SAGEConv(4, 2, 'mean')\n",
    "}\n",
    "\n",
    "HetroConv = HeteroGraphConv(mods, \n",
    "                            in_feats,\n",
    "                            out_feats,\n",
    "                            aggregate='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d9ab69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = HetroConv(g, g_feat)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ff313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
